\documentclass{bmvc2k}

%% Enter your paper number here for the review copy
\bmvcreviewcopy{??}

\title{3D Object Detection and tracking in Point Cloud Flow}

% Enter the paper's authors in order
% \addauthor{Name}{email/homepage}{INSTITUTION_CODE}
\addauthor{Xusen Guo}{guoxs3@mail2.sysu.edu.cn}{1}
\addauthor{Kai Huang}{huangk36@mail.sysu.edu.cn}{1}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
	Key Laboratory of Machine Intelligence\\
	and Advanced Computing, Ministry of \\
	Education School of Data and Computer Science, Sun Yat-Sen \\
	University, GuangZhou, China
}

\runninghead{XuSen Guo, KaiHuang, SYSU}{3D Object Detection and tracking in Point Cloud Flow}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\maketitle

\begin{abstract}
Recent approaches for 3D object detection has made great progresses due to the evolution in deep learning. However, previous works are mostly based on single frame point cloud or image, information between point cloud frames is almost not utilized. In this paper, we try to leverage the temporal information in point cloud flow and explore 3D object detection and tracking based on 3D data flow. Towards this goal, we set up a ConvNet architecture that can associate multi-frame image and LiDAR data to produce accurate 3D detection boxes and trajectories. Notably, a correlation module is introduced to capture object co-occurrences across time, and a multi-task objective for frame-based object detection and across-frame track regression is used, therefore, the network can performs detection and tracking simultaneously. Our proposed architecture is shown to produce competitive results on the KITTI Object tracking datasets. Code and models will be available soon.
\end{abstract}

%-------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
A lot of attention has been paid to 3D object detection because of the rapid development of  autonomous driving industry in recent years. Although object detection in images has made tremendous progress due to the emergence of deep learning \cite{krizhevsky2012imagenet, simonyan2014very, he2016deep} and their region based descendants \cite{dai2016r, girshick2015fast, ren2015faster}, extending 2D approaches to 3D scene is extremely hard. This is mainly because of the curse of dimensionality and the sparsity of 3D data. 

In spite of the difficulty in 3D object detection, many works have been carried out. Recent approaches are usually done in three fronts: image based, point cloud based and fusion of image and point cloud. These methods have achieved remarkable performance but are limited to single frame input, applying existing detection networks on individual frames will loss the consistency and difference between frame and introduce unaffordable computational cost for most applications.

Compare to single point cloud frame, point cloud flow data is more natural and straightforward for most situation. Thus Fast and accurate point cloud flow object detection is crucial for autonomous driving. Similar to extend 2D object detection methods to 3D situation, we also can extend video object detection approaches to 3D point cloud flow object detection. 

Most modern computer vision approaches to video object detection require flow estimation, which is a fundamental task in video analysis. For example, a series work in \cite{zhu2017flow,zhu2018towards} associate vision feature and optical flow to build an accurate and end-to-end learning framework for video object detection. However, applying video object detection framework to autonomous driving is hard, because video object detection suffers from motion blur and partial occlusion, which are conventional in driving scenarios. While another choice is to utilize LiDAR data, since point cloud provides an accurate spatial representation of the world allowing for precise positioning of objects of interest, motion blur and occlusion problem can be avoided in 3D object detection. However, LiDAR does not capture appearance well when compared to the richness of images, moreover, accurate 3D scene flow estimation from point clouds is tremendously tough. Although some approaches such as \cite{liu2018learning, behl2018pointflownet} have been presented to learn 3D motion field in the world, it is hard to implementation in large scenes. Thus the challenge is, can we propose an approach that accurately do 3D object detection in point cloud flow without 3D scene flow?

Inspired by \cite{feichtenhofer2017detect}, we transform AVOD \cite{ku2018joint} structure into 
a two stream network embedding with a correlation module, named Bi-AVOD, which takes two adjacent key frames as input and predicts location and orientation of object as well as their local displacement. Noting that Bi-AVOD is an aggregate view object detection architecture capable of fusing different features in image and point cloud, thus its input includes two adjacent images in front view and two adjacent BEV (bird eye view) from LIDAR data. While the correlation module compute convolutional cross-correlation between the feature responses of adjacent key frames to estimate displacement of the same objects. With local displacement and object orientation, object location in intermediate frames can be calculated by interpolation. Moreover, detections can be linked between frames with the help of local displacement and multiple object tracking can be performed through \textit{tracking by detection} \cite{lenz2015followme}. 

In summary, our contributions are threefold: \textit{(i)} we set up a two stream architecture based on AVOD for simultaneous 3D object detection and tracking; \textit{(ii)} we introduce correlation features to capture object co-occurrences across time and perform frame-level detections in a high speed through interpolation; \textit{(iii)} we utilize tracking result to improve detection performance and preliminary explore the algorithm for key frame selection. 
%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:related work}
\subsection{Video object detection}
Video object detection in image has received increased attention since ImageNet VID datasets introduced. Most approaches for video object detection utilize optical flows, which present temporal information in videos. Some representative work such as FGFA \cite{zhu2017flow}, it leverages temporal coherence on feature level, and improves the pre-frame features by aggregation of nearby features along the motion paths with the help of optical flows. Later a more efficient approach based on \cite{zhu2017flow} has been presented in \cite{zhu2018towards}, it introduces three new techniques: sparsely recursive feature aggregation, spatially-adaptive partial feature updating and temporally-adaptive key frame scheduling, which make this unified approach faster, more accurate and more flexible.

There are also some approach try to learn temporal information between consecutive frames. D\&T \cite{feichtenhofer2017detect} set up a ConvNet architecture for simultaneous detection and tracking in video. In order to capture cross-occurrences across time, it aid a correlation operation in networks. Our Bi-AVOD architecture mainly inspired by this work.

\subsection{3D object detection}
Currently, most approaches in 3D object detection can be divided into three types: image based detectors, LiDAR based detectors and fusion based detectors.Image based approaches such as Mono3D\cite{7780605}, 3DOP\cite{chen20183d} use camera data only, since image has limited depth information, specific hand-crafted geometric features are required. LiDAR based methods are usually done in two fronts, one is utilizing a voxel grid representation to encoder point cloud and applying 3D CNN for features extracted, these approaches including 3D FCN \cite{li20173d}, Vote3Deep \cite{engelcke2017vote3deep} and VoxelNet \cite{zhou2018voxelnet} \etal, these approaches suffer from the sparsity of point cloud and enormous computation cost in 3D convolution; others LIDAR based methods try to project point cloud to bird eye view (BEV) and apply 2D CNN for object detection, such as PIXOR\cite{yang2018pixor}, FaF\cite{luo2018fast} and Comple-YOLO \cite{simon2018complex} \etal These methods take advantage of the fact that objects in autonomous driving almost on the same plane thus loss of height information has little affect to the result, while the depth and Geometric information can be retained and computational complexity reduced significantly, making real-time detection possible. However, due to the sparsity of point cloud, the feature information after projecting is insufficient for accurate object detection especially for the small target.

There are also many multi-modal fusion methods that combine images and LiDAR data to improve detection accuracy. F-PointNet \cite{qi2018frustum} first extracts the 3D bounding frustum of an object by extruding 2D bounding boxes from image detectors, then consecutively perform 3D object instance segmentation and amodal extent regression to estimate the amodal 3D bounding box. MV3D\cite{chen2017multi} extends the image based RPN of Faster R-CNN\cite{ren2015faster} to 3D and proposes a 3D RPN targeted at autonomous driving scenarios. MV3D uses every pixel in BEV feature map to multiple 3D anchors and then feeds the anchor to RPN to generate 3D proposals that are used to create view-specific feature crops from BEV feature maps and images. A deep fusion scheme is used to combine information from these feature crops to produce final detection output. However, MV3D does not work well for small targets due to the insufficient data for feature extracting caused by downsampling in convolutional feature extractors. AVOD\cite{ku2018joint} architecture is similar to MV3D in 3D RPN and feature fusion, however, its feature extract provides full resolution feature maps thus show greatly help in localization accuracy for small targets during the second stage of the detection framework. Our proposed architecture mostly based on AVOD mention above.

\subsection{3D object tracking}
More and more work has been done in 3D object tracking based on tracking by detection due to the rapidly development in 3D object detection. These approaches usually trend to apply 3D object detection and tracking simultaneously. FaF \cite{luo2018fast} jointly reasons about 3D detection, tracking and motion forecasting taking a 4D tensor created from multiple consecutive temporal frames. The most similar approach to our work is \cite{frossard2018end}, however, their 3D detector is based on MV3D while ours is AVOD, and their detections association is done by solving a linear program after passing to a matching net and scoring net, while ours use a extending IOU based algorithm \cite{bochinski2018extending} by leveraging corresponding displacements over time.
%-------------------------------------------------------------------------
\section{Methodology}
\label{sec:method}
In this section we first present the problem statement of point cloud flow detection and tracking (Sec. 3.1), then we will give an overview of the Bi-AVOD approach (Sec. 3.2) that generates detections and tracklets give two adjacent key frames as input. We then introduce the correlation module (Sec. 3.3) that aid the network in the tracking process. Sec. 3.4 shows how we link detections to trajectories and Sec. 3.5 simply introduces evaluation metrics in multiple object tracking.

\subsection{Problem formulation}
Give a streaming point cloud of $N$ frames $\{I_f\}$ for $f \in \{1, ... N\}$, the stream object detection task requires a set of detections $D_f$ for each frame $I_f$. Each detection set consists of object detections $\{D^i_f\}$ while $i \in \{1,...N_f\}$ ($N_f$ is the number of detections in frame $f$).  

\subsection{Bi-AVOD model structure}


\subsection{Correlation module for object tracking}


\subsection{Link detections to trajectories}

\subsection{Metrics}

%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{KITTI tracking datasets}


\subsection{Data preprocessing}


\subsection{Training and testing}

\subsection{Results}

%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}


%-------------------------------------------------------------------------
\bibliography{egbib}
\end{document}
