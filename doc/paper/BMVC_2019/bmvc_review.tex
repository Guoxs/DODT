\documentclass{bmvc2k}

%% Enter your paper number here for the review copy
\bmvcreviewcopy{??}

\title{3D Object Detection and tracking in Point Cloud Flow}

% Enter the paper's authors in order
% \addauthor{Name}{email/homepage}{INSTITUTION_CODE}
\addauthor{Xusen Guo}{guoxs3@mail2.sysu.edu.cn}{1}
\addauthor{Kai Huang}{huangk36@mail.sysu.edu.cn}{1}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
	Key Laboratory of Machine Intelligence\\
	and Advanced Computing, Ministry of \\
	Education School of Data and Computer Science, Sun Yat-Sen \\
	University, GuangZhou, China
}

\runninghead{XuSen Guo, KaiHuang, SYSU}{3D Object Detection and tracking in Point Cloud Flow}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\usepackage{amsfonts,amssymb} 

%-------------------------------------------------------------------------
% Document starts here
\begin{document}
\maketitle
\begin{abstract}
Recent approaches for 3D object detection has made great progresses due to the evolution in deep learning. However, previous works are mostly based on single frame point cloud or image, information between point cloud frames is almost not utilized. In this paper, we try to leverage the temporal information in point cloud flow and explore 3D object detection and tracking based on 3D data flow. Towards this goal, we set up a ConvNet architecture that can associate multi-frame image and LiDAR data to produce accurate 3D detection boxes and trajectories. Notably, a correlation module is introduced to capture object co-occurrences across time, and a multi-task objective for frame-based object detection and across-frame track regression is used, therefore, the network can performs detection and tracking simultaneously. Our proposed architecture is shown to produce competitive results on the KITTI Object tracking datasets. Code and models will be available soon.
\end{abstract}

%-------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
A lot of attention has been paid to 3D object detection because of the rapid development of  autonomous driving industry in recent years. Although object detection in images has made tremendous progress due to the emergence of deep learning \cite{krizhevsky2012imagenet, simonyan2014very, he2016deep} and their region based descendants \cite{dai2016r, girshick2015fast, ren2015faster}, extending 2D approaches to 3D scene is extremely hard. This is mainly because of the curse of dimensionality and the sparsity of 3D data. 

In spite of the difficulty in 3D object detection, many works have been carried out. Recent approaches are usually done in three fronts: image based, point cloud based and fusion of image and point cloud. These methods have achieved remarkable performance but are limited to single frame input, applying existing detection networks on individual frames will loss the consistency and difference between frame and introduce unaffordable computational cost for most applications.

Compare to single point cloud frame, point cloud flow data is more natural and straightforward for most situation. Thus Fast and accurate point cloud flow object detection is crucial for autonomous driving. Similar to extend 2D object detection methods to 3D situation, we also can extend video object detection approaches to 3D point cloud flow object detection. 

Most modern computer vision approaches to video object detection require flow estimation, which is a fundamental task in video analysis. For example, a series work in \cite{zhu2017flow,zhu2018towards} associate vision feature and optical flow to build an accurate and end-to-end learning framework for video object detection. However, applying video object detection framework to autonomous driving is hard, because video object detection suffers from motion blur and partial occlusion, which are conventional in driving scenarios. While another choice is to utilize LiDAR data, since point cloud provides an accurate spatial representation of the world allowing for precise positioning of objects of interest, motion blur and occlusion problem can be avoided in 3D object detection. However, LiDAR does not capture appearance well when compared to the richness of images, moreover, accurate 3D scene flow estimation from point clouds is tremendously tough. Although some approaches such as \cite{liu2018learning, behl2018pointflownet} have been presented to learn 3D motion field in the world, it is hard to implementation in large scenes. Thus the challenge is, can we propose an approach that accurately do 3D object detection in point cloud flow without 3D scene flow?

Inspired by \cite{feichtenhofer2017detect}, we transform AVOD \cite{ku2018joint} structure into 
a two stream network embedding with a correlation module, named Bi-AVOD, which takes two adjacent key frames as input and predicts location and orientation of object as well as their local displacement. Noting that Bi-AVOD is an aggregate view object detection architecture capable of fusing different features in image and point cloud, thus its input includes two adjacent images in front view and two adjacent BEV (bird eye view) from LIDAR data. While the correlation module compute convolutional cross-correlation between the feature responses of adjacent key frames to estimate displacement of the same objects. With local displacement and object orientation, object location in intermediate frames can be calculated by interpolation. Moreover, detections can be linked between frames with the help of local displacement and multiple object tracking can be performed through \textit{tracking by detection} \cite{lenz2015followme}. 

In summary, our contributions are threefold: \textit{(i)} we set up a two stream architecture based on AVOD for simultaneous 3D object detection and tracking; \textit{(ii)} we introduce correlation features to capture object co-occurrences across time and perform frame-level detections in a high speed through interpolation; \textit{(iii)} we utilize tracking result to improve detection performance and preliminary explore the algorithm for key frame selection. 
%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:related work}
\subsection{Video object detection}
Video object detection in image has received increased attention since ImageNet VID datasets introduced. Most approaches for video object detection utilize optical flows, which present temporal information in videos. Some representative work such as FGFA \cite{zhu2017flow}, it leverages temporal coherence on feature level, and improves the pre-frame features by aggregation of nearby features along the motion paths with the help of optical flows. Later a more efficient approach based on \cite{zhu2017flow} has been presented in \cite{zhu2018towards}, it introduces three new techniques: sparsely recursive feature aggregation, spatially-adaptive partial feature updating and temporally-adaptive key frame scheduling, which make this unified approach faster, more accurate and more flexible.

There are also some approach try to learn temporal information between consecutive frames. D\&T \cite{feichtenhofer2017detect} set up a ConvNet architecture for simultaneous detection and tracking in video. In order to capture cross-occurrences across time, it aid a correlation operation in networks. Our Bi-AVOD architecture mainly inspired by this work.

\subsection{3D object detection}
Currently, most approaches in 3D object detection can be divided into three types: image based detectors, LiDAR based detectors and fusion based detectors.Image based approaches such as Mono3D\cite{7780605}, 3DOP\cite{chen20183d} use camera data only, since image has limited depth information, specific hand-crafted geometric features are required. LiDAR based methods are usually done in two fronts, one is utilizing a voxel grid representation to encoder point cloud and applying 3D CNN for features extracted, these approaches including 3D FCN \cite{li20173d}, Vote3Deep \cite{engelcke2017vote3deep} and VoxelNet \cite{zhou2018voxelnet} \etal, these approaches suffer from the sparsity of point cloud and enormous computation cost in 3D convolution; others LIDAR based methods try to project point cloud to bird eye view (BEV) and apply 2D CNN for object detection, such as PIXOR\cite{yang2018pixor}, FaF\cite{luo2018fast} and Comple-YOLO \cite{simon2018complex} \etal These methods take advantage of the fact that objects in autonomous driving almost on the same plane thus loss of height information has little affect to the result, while the depth and Geometric information can be retained and computational complexity reduced significantly, making real-time detection possible. However, due to the sparsity of point cloud, the feature information after projecting is insufficient for accurate object detection especially for the small target.

There are also many multi-modal fusion methods that combine images and LiDAR data to improve detection accuracy. F-PointNet \cite{qi2018frustum} first extracts the 3D bounding frustum of an object by extruding 2D bounding boxes from image detectors, then consecutively perform 3D object instance segmentation and amodal extent regression to estimate the amodal 3D bounding box. MV3D\cite{chen2017multi} extends the image based RPN of Faster R-CNN\cite{ren2015faster} to 3D and proposes a 3D RPN targeted at autonomous driving scenarios. MV3D uses every pixel in BEV feature map to multiple 3D anchors and then feeds the anchor to RPN to generate 3D proposals that are used to create view-specific feature crops from BEV feature maps and images. A deep fusion scheme is used to combine information from these feature crops to produce final detection output. However, MV3D does not work well for small targets due to the insufficient data for feature extracting caused by downsampling in convolutional feature extractors. AVOD\cite{ku2018joint} architecture is similar to MV3D in 3D RPN and feature fusion, however, its feature extract provides full resolution feature maps thus show greatly help in localization accuracy for small targets during the second stage of the detection framework. Our proposed architecture mostly based on AVOD mention above.

\subsection{3D object tracking}
More and more work has been done in 3D object tracking based on tracking by detection due to the rapidly development in 3D object detection. These approaches usually trend to apply 3D object detection and tracking simultaneously. FaF \cite{luo2018fast} jointly reasons about 3D detection, tracking and motion forecasting taking a 4D tensor created from multiple consecutive temporal frames. The most similar approach to our work is \cite{frossard2018end}, however, their 3D detector is based on MV3D while ours is AVOD, and their detections association is done by solving a linear program after passing to a matching net and scoring net, while ours use a extending IOU based algorithm \cite{bochinski2018extending} by leveraging corresponding displacements over time.
%-------------------------------------------------------------------------
\section{Methodology}
\label{sec:method}
In this section we first give an overview of the Bi-AVOD approach (Sec. 3.1) that generates detections and tracklets given two adjacent key frames as input. We then introduce the correlation module (Sec. 3.2) that aiding the network in the tracking process. Sec. 3.3 shows the multi-task objective function and Sec. 3.4 shows how we implement 3D streaming detection and tracking using prediction results of Bi-AVOD.

\begin{figure}
	\rule{0pt}{1ex}
	\setlength{\abovecaptionskip}{-1.0cm}
	\begin{center}
		\includegraphics[trim={0cm, 10cm, 3cm, 1cm}, clip, width=\textwidth]{images/Bi-AVOD.pdf}
	\end{center}
	\caption{Bi-AVOD architecture}
	\label{fig:bi-avod}
	\vspace{-0.4cm}
\end{figure}

\subsection{Bi-AVOD model structure}

\begin{figure}
	\setlength{\abovecaptionskip}{-0.5cm}
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim={0cm, 8cm, 2cm, 0cm}, clip, width=2.2in]{images/AVOD.pdf}
		\caption{AVOD architecture}
		\label{fig:avod}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim={0cm, 9cm, 2cm, 0cm}, clip, width=2.2in]{images/Correlation.pdf}
		\caption{Correlation module}
		\label{fig:correlation}
	\end{minipage}
\end{figure}

We aim at performing 3D object detection in point cloud flow, and later apply multi-object tracking using tracking by detection paradigm. We build our Bi-AVOD mainly based on AVOD\cite{ku2018joint}, \figurename .\ref{fig:bi-avod} illustrates our Bi-AVOD architecture. By doubling its input field, we can feed two adjacent key frames simultaneously and obtained corresponding object detection results. Meanwhile, the local displacements can be estimated by computing convolutional cross-correlation between the feature responses of adjacent frames using correlation module. With the predicted bounding boxes of two adjacent key frames and their local displacements, we can implement interpolation algorithms to generate interval bounding boxes for 3D flow detection and develop data association algorithms for 3D object tracking. Note that two \textit{Feature Extractor} and two \textit{AVOD module} in \figurename .\ref{fig:bi-avod} are parameter sharing, thus the increased computational cost comes only from the correlation module. We will simply review the AVOD architecture in this section, and the correlation module will leave to the next section.

AVOD architecture is proposed for 3D object detection in autonomous driving by aggregating front view image and bird eye view (BEV) feature maps (generated by LiDAR data). It is a two stage method and \figurename .\ref{fig:avod} illustrates its architecture. Firstly, it uses feature pyramids based extractors to generate full resolution feature maps from both BEV map and RGB image. Both feature maps are then feed to fusion RPN to generate \textit{Top K} non-oriented region proposals after applied a $1 \times 1$ convolutional layer for dimensionality reduction, note that the default anchors in image and BEV map are generated through 3D anchor grid projection, and ROI Pooling is implemented by \textit{Crop and Resize} operations. Finally, the \textit{Top K} proposals are passed to the detection network for dimension refinement, orientation estimation and category classification. We refer
the reader to \cite{ku2018joint} for an further explanation of the architecture.

\subsection{Correlation module}
The correlation module is illustrated in \figurename .\ref{fig:correlation}. It first performs correlation on two image features and two BEV map features respectively. Then similar to the above mentioned RPN, it extracts feature crops via multiview \textit{Crop and Resize} operations guided by \textit{Top K} non-oriented region proposals. The feature crops then feed to fusion module and multiview aggregated features are generated. While the fusion module is identical to the one mentioned in AVOD, which first introduced in MV3D\cite{chen2017multi}, and includes three different fusion schemes, \textit{early fusion}, \textit{late fusion} and \textit{deep fusion}. Finally, the aggregated features are passed to a fully connected layer for regression after NMS.

Similar to FlowNet \cite{dosovitskiy2015flownet}, we restrict correlation to a local neighborhood instead of all possible circular shifts in a feature map, which avoids large output dimensionality and too large displacements. The correlation operation performs point-wise feature comparison of two feature maps $f_t$, $f_{t+\tau}$ by

\begin{equation}
\mathcal{C}^{t, t+\tau}(i, j, p, q) = \Big \langle f_t(i, j), f_{t+\tau} (i+p, j+q) \Big \rangle 
\end{equation}

where $p, q \in [-d, d]$ are offsets to compare features in a local square window defined by the maximum displacement $d$, and $i, j$ are the location of window center in feature map. The output is a correlation feature map of size $\mathcal{C} \in \mathbb{R}^{h \times w \times (2d+1) \times (2d+1)}$ where $h, w$ are the height, width of the feature map.

After above operation we get two correlation feature maps, one for point cloud $\mathcal{C}^{t, t+\tau}_{pc}$, and another for RGB image $\mathcal{C}^{t, t+\tau}_{img}$. Later ROI pooling and \textit{early fusion} are performed just as in detection part. Aggregate feature $\mathcal{C}^{t,t+\tau}_{fusion} = \frac{1}{2}(\mathcal{C}^{t, t+\tau}_{pc} + \mathcal{C}^{t, t+\tau}_{img})$ is then flatten and fed to a fully connected layer to predict the transformation
 $\Delta^{t, t+\tau} = (\Delta^{t,t+\tau}_{x, y, z}, \Delta^{t,t+\tau}_{w, h, l},\Delta^{t,t+\tau}_{r}) \in \mathbb{R}^7$ of the RoIs from $t$ to $t+\tau$. 

\subsection{Multitask detection and correlation objective}
We extend the multi-task loss of object detection, consisting of a classification loss $L_{cls}$ and a regression loss $L_{reg}$, with an additional term $L_{corr}$ that scores the displacement regression between objects across two frames. Considering a batch of $N$ RoIs after category balanced sampling, the network predicts softmax probabilities $\{p_i\}^N_{i=1}$, bounding box regression offsets $\{b_i\}^N_{i=1}$, and cross-frame displacement regression $\{\Delta^{t+\tau}_i\}^{N_{corr}}_{i=1}$, the overall objective function is shown as:

\begin{equation}
\begin{split}
L(\{p_i\}, \{b_i\}, \{\Delta_i\}) = \frac{1}{N} \sum^N_{i=1} L_{cls}(p_{i, c^*}) 
+ \frac{\alpha}{N_{fg}}\sum^N_{i=1} [c^*_i > 0]L_{reg}(b_i, b^*_i) \\
+ \frac{\beta}{N_{corr}} \sum^{N_{corr}}_{i=1}L_{corr}(\Delta^{t+\tau}_i, \Delta^{*, t+\tau}_i)
\end{split}
\end{equation}

where $c^*_i$ is the ground truth class label of an RoI and $p_{i, c^*_i}$ is corresponding predicted softmax score. $b^*_i$ is the ground truth bounding box regression target, and $\Delta^{*, t+\tau}_i$ is the displacement regression target. The indicator function $ [c^*_i > 0]$ is 1 for foreground RoIs and 0 for background RoIs. $L_{cls})$ is cross-entropy loss, and $L_{reg}$, $L_{corr}$ are smooth L1 loss \cite{girshick2015fast}. $\alpha$ and $\beta$ are weight for $L_{reg}$ and $L_{corr}$. Note that we only consider the $N_{fg}$ foreground RoIs loss for $L_{reg}$, and $L_{corr}$ is active only for foreground RoIs which have a track correspondence across two key frames. Additionally, For a single target $D_t = (D^{x,y,z}_t, D^{w,h,l}_t, D^r_t) \in \mathbb{R}^7$ in frame $t$ and its track correspondence $D_{t+\tau}$ in frame $t+\tau$, the displacement regression values for the target $ \Delta^{*, t+\tau}_i$ are encoded just like the 3D bounding box in \cite{ku2018joint}.

\subsection{3D streaming detection and tracking}
Give a streaming point cloud of $N$ frames $\{I_f\}$ for $f \in \{1, ... N\}$, the streaming object detection task needs to predict a set of detections $D_f$ for each frame $I_f$. Each detection set $D_f$ consists of object detections $\{D^i_f\}$ while $i \in \{1,...N_f\}$ ($N_f$ is the number of detections in frame $f$). Note that $D_f$ can also be an empty set when no object is detected in a frame. In 3D object detection, Each detection $D^i_f$ is parametrized as $D^i_f = (x^i_f, y^i_f, z^i_f, w^i_f, h^i_f, l^i_f, \theta^i_f, s^i_f)$, where $(x^i_f, y^i_f, z^i_f)$ corresponds to the center (bottom center in KITTI\cite{geiger2013vision} Datasets) of the detection box in point cloud, $(w^i_f, h^i_f, l^i_f)$ corresponds to width, height, length of the detection box, $\theta^i_f$ is the rotation angle in yaw axis and $s^i_f$ is the detectors confidence in the bounding box.

Because of the redundant features in streaming, we can only compute the object detections in key frames, while the detections in intermediate frames can be calculated using the detections in adjacent two key frames. Support we have two predicted object detections set $(D_f, D_{f+\tau})$ in two consecutive key frames $(I_f, I_{f+\tau})$ where $\tau$ is temporal stride, the detection results $\{D^i_{f+t}\}$ $(t \in \{1, \tau-1\})$ in intermediate frame $D_{f+t}$ can be obtained by:

\begin{equation}
D^i_{f+t} = \mathcal{F}(W_f D^i_f, W_{f+\tau} D^i_{f+\tau})
\end{equation}

where $W$ is corresponding weight and $\mathcal{F}$ is generation function to produce $D^i_{f+t}$, in this paper we use quadratic interpolation function. Note that we can utilize (1) to generate $D^i_{f+t}$ only when the target exists in $D_f$ and $D_{f+\tau}$ simultaneously. if the target is emerged or end in intermediate frames, this method would be failed. Though we can develop the key frames selection function carefully to handle this situation, it is beyond the scope of this article. In this paper we focus on the targets that always exist between two key frames.

With object detections in each frame, multi-object tracking can be implemented by tracking-by-detection paradigm. For each bounding box in each frame, MOT try to associate it to a unique target trajectory $T_k = \{D^k_{f_1}, D^k_{f_2}, ..., D^k_{f_{N_k}}\}$, where $k$ is trajectory id and $N_k$ is the length of $T_k$, $\{f_1, f_2, ..., f_{N_k}\}$ are corresponding frames id.

%-------------------------------------------------------------------------
\section{Experimental evaluation}
\label{sec:experiments}

\subsection{Datasets}
We use the KITTI object tracking Benchmark \cite{geiger2013vision} for evaluation. It consists of 21 training sequences and 29 test sequences with vehicles annotated in 3D. Each sequence includes hundreds of point clouds frames captured by Velodyne HDL-64E rotating 3D laser scanner and corresponding RGB images. We split 21 training sequences into two parts based on the parity of the sequence number, odd number for training and even number for evaluation. For testing ,we train our model in all 21 training sequences.

\subsection{Experiments}


%-------------------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}


%-------------------------------------------------------------------------
\bibliography{egbib}
\end{document}
